{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruning_test_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DWAXFoba8P9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWiF4L5I8WPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful functions.\n",
        "\n",
        "# Function to reinit the net's weights to the original weights + apply the \n",
        "# current mask.\n",
        "\n",
        "# TODO: see if you need to make weight_orig requires_grad=False.\n",
        "def reinit_and_apply_mask(net):\n",
        "    # For each layer, set the current weights as weight_init, leave the mask untouched.\n",
        "    modules_names = net._modules.keys()\n",
        "    for module_name in modules_names:\n",
        "        \n",
        "        # Check if module_name has been pruned.\n",
        "        pruned = False\n",
        "        for name, param in net._modules[module_name].named_parameters():\n",
        "            if 'weight_orig' in name:\n",
        "                pruned = True\n",
        "\n",
        "        # If the module has been pruned, copy the weights from weight_orig \n",
        "        # to module weights and apply the mask.\n",
        "        net._modules[module_name].weight = net._modules[module_name].weight_orig.detach().requires_grad_()\n",
        "        net._modules[module_name].weight = net._modules[module_name].weight * net._modules[module_name].weight_mask\n",
        "\n",
        "\n",
        "# Function for weight initialisation (taken from \n",
        "# https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py)\n",
        "def weight_init(m):\n",
        "    '''\n",
        "    Usage:\n",
        "        model = Model()\n",
        "        model.apply(weight_init)\n",
        "    '''\n",
        "    if isinstance(m, nn.Conv1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm3d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.LSTMCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRU):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRUCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "\n",
        "# Code for training one epoch (one pass through the dataset).\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
        "    model.train() #Sets nn.Module in train mode (has effects only on some models)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "        output = model(imgs)\n",
        "        \n",
        "        train_loss = criterion(output, targets)\n",
        "        train_loss.backward()\n",
        "\n",
        "        # In original, gradient of the pruned nodes were made 0.\n",
        "        optimizer.step()\n",
        "    \n",
        "    # train_loss is a tensor with one value;\n",
        "    # tensor.item() returns the value held by a tensor with one value;\n",
        "    return train_loss.item()\n",
        "\n",
        "# Not sure if the loss calculation is correct here.\n",
        "def calculate_accuracy_and_loss(model, loader, criterion):\n",
        "    # Put the model in evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    nr_batches = len(loader)\n",
        "    total_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, targets) in enumerate(loader):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            output = model(imgs)\n",
        "\n",
        "            total_loss += (1/nr_batches) * criterion(output, targets)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += targets.shape[0]\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = correct/total * 100\n",
        "    return accuracy, total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvcYvhpabvkm",
        "colab_type": "code",
        "outputId": "f2b6366f-9f1f-40d3-f1bf-bc154629bf88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        }
      },
      "source": [
        "# Test run #1:\n",
        "\n",
        "# All the variables + initialisations we need for an experiment.\n",
        "reinit = True\n",
        "dataset = \"mnist\"\n",
        "batch_size = 32\n",
        "experiment_name = \"weight_magnitude_pruning_test\"\n",
        "# Not sure if we need a validation set.\n",
        "nr_valid_elems = 10000\n",
        "test_freq = 1\n",
        "# model_type = \"LeNetTrash\"\n",
        "# Will we experiment with different optimizers/losses/arguments to those?\n",
        "# If yes, we need to prove a re-initializer.\n",
        "#optimizer = \n",
        "#criterion = \n",
        "prune_iterations = 5\n",
        "epochs = 5\n",
        "#criterion = \n",
        "# Need to have a parameter for the pruning type + python file with the pruning.\n",
        "# WHat if the pruning changes parameters iteration after iteration?\n",
        "# Then modify the script accordingly.\n",
        "# What if the pruning method changes iteration after iteration?\n",
        "# Load a list of pruning methods from the file instead? - to be seen.\n",
        "\n",
        "# All the experiment data must be saved in a folder experiment_name.\n",
        "# The data must be saved in a different folde (outside any experiment folder).\n",
        "\n",
        "writer = SummaryWriter()\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# TODO: Add more dataset-dependent data loaders.\n",
        "if dataset == 'mnist':\n",
        "    # Transforms which will be applied to the data.\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081, ))\n",
        "                                    ])\n",
        "    # 0.1307, 0.3081 represent the mean + std of the mnist dataset.\n",
        "    \n",
        "    # Split the train dataset into a train + valid datasets.\n",
        "    # Must set the values of the samples in each split (here, 50000, 10000).\n",
        "    dataset = datasets.MNIST(root=os.getcwd() + '/data', train=True, download=True, transform=transform)\n",
        "    train_set, valid_set = torch.utils.data.random_split(dataset, [50000, 10000])\n",
        "\n",
        "    # Load the test dataset.\n",
        "    test_set = datasets.MNIST(root=os.getcwd() + '/data', train=False, transform=transform)\n",
        "\n",
        "    # TODO: Also, import the relevant models (to be tested).\n",
        "    # from models.mnist import fc1\n",
        "\n",
        "\n",
        "# Transformations will not be applied until you call a DataLoader on it.\n",
        "# make valid_loader = test_loader if the option for the split is 0.\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
        "\n",
        "# Load model (placeholder, see if it makes sense to load models given how PruningMethods differ).\n",
        "#if model_type == \"fc1\":\n",
        "    #model = fc1.fc1().to(device)\n",
        "# Create a neural net.\n",
        "class LeNetTrash(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetTrash, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.fc2 = nn.Linear(300, 100)\n",
        "        self.fc3 = nn.Linear(100, 10)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = input.flatten(start_dim=1, end_dim=-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = LeNetTrash().to(device)\n",
        "\n",
        "# Initialise weights.\n",
        "model.apply(weight_init)\n",
        "\n",
        "# Save initial model for reference.\n",
        "os.makedirs(os.getcwd() + '/' + experiment_name, exist_ok=True)\n",
        "torch.save(model, os.getcwd()+ '/' + experiment_name + '/model_init.pth')\n",
        "\n",
        "# Main training loop. We can add bells and whistles afterwards, depending on \n",
        "# what we want to save.\n",
        "\n",
        "# Placeholder for loading pruning methods.\n",
        "# TODO: refactor after testing.\n",
        "def prune_model(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=0.1)\n",
        "\n",
        "# TODO: remove these two?\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter(os.getcwd() + '/' + experiment_name)\n",
        "\n",
        "best_accuracies = np.zeros((prune_iterations, ))\n",
        "best_accuracy = 0\n",
        "for prune_iteration in range(prune_iterations):\n",
        "    print('\\n\\nStarting pruning iteration: ' + str(prune_iteration) + '\\n')\n",
        "\n",
        "    #os.makedirs(os.getcwd() + '/' + experiment_name + '/' + 'prune_iter_' + str(prune_iteration), exist_ok=True)\n",
        "\n",
        "    if prune_iteration != 0:\n",
        "        # do the pruning here.\n",
        "        # Before pruning (or after, we can save the mask here, if interested).\n",
        "        prune_model(model)\n",
        "        if reinit:\n",
        "            # do reinitialization of the net + optimizer here.\n",
        "            reinit_and_apply_mask(model)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "\n",
        "        if epoch % test_freq == 0:\n",
        "            experiment_PATH = os.getcwd() + '/' + experiment_name\n",
        "            # save whatever you're interested in here\n",
        "            # need a function to save a model with a name\n",
        "            # for starters, save best model for each pruning iteration\n",
        "\n",
        "            train_acc, train_loss = calculate_accuracy_and_loss(model, train_loader, criterion)\n",
        "            valid_acc, valid_loss = calculate_accuracy_and_loss(model, valid_loader, criterion)\n",
        "\n",
        "            writer.add_scalar('Accuracy/train_' + str(prune_iteration), train_acc, epoch)\n",
        "            writer.add_scalar('Loss/train_' + str(prune_iteration), train_loss, epoch)\n",
        "            writer.add_scalar('Accuracy/valid_' + str(prune_iteration), valid_acc, epoch)\n",
        "            writer.add_scalar('Loss/valid_' + str(prune_iteration), valid_loss, epoch)\n",
        "\n",
        "            # Maybe save the best models here, now we're interested only in best accs/losses.\n",
        "            if(valid_acc > best_accuracy):\n",
        "                best_accuracy = valid_acc\n",
        "                #PATH = get_best_model_PATH(experiment_PATH, best_model_filename, 0)\n",
        "                #remove_checkpoint(PATH)\n",
        "                #save_checkpoint(PATH, epoch, net, optimizer)\n",
        "\n",
        "            print('Epoch: ' + str(epoch) +  ', Train loss: {:.4f}, Train Acc: {:.2f}, Valid loss: {:.4f}, Valid Acc: {:.2f}'.format(train_loss, train_acc, valid_loss, valid_acc))\n",
        "    \n",
        "    best_accuracies[prune_iteration] = best_accuracy\n",
        "\n",
        "    # Here, the model has finished training. \n",
        "    # Again, save whatever information seems legit to save, dependent on the pruning run.\n",
        "    # Save a list of best accuracies for each pruning iteration.\n",
        "    # Cool, that's all I think."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type LeNetTrash. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Starting pruning iteration: 0\n",
            "\n",
            "Epoch: 0, Train loss: 0.1221, Train Acc: 96.02, Valid loss: 0.1459, Valid Acc: 95.55\n",
            "Epoch: 1, Train loss: 0.0642, Train Acc: 97.89, Valid loss: 0.0981, Valid Acc: 96.96\n",
            "Epoch: 2, Train loss: 0.0634, Train Acc: 97.96, Valid loss: 0.1051, Valid Acc: 96.86\n",
            "Epoch: 3, Train loss: 0.0388, Train Acc: 98.83, Valid loss: 0.0795, Valid Acc: 97.60\n",
            "Epoch: 4, Train loss: 0.0400, Train Acc: 98.66, Valid loss: 0.0924, Valid Acc: 97.38\n",
            "\n",
            "\n",
            "Starting pruning iteration: 1\n",
            "\n",
            "Epoch: 0, Train loss: 0.0412, Train Acc: 98.53, Valid loss: 0.0983, Valid Acc: 97.06\n",
            "Epoch: 1, Train loss: 0.0361, Train Acc: 98.73, Valid loss: 0.1011, Valid Acc: 97.37\n",
            "Epoch: 2, Train loss: 0.0305, Train Acc: 98.94, Valid loss: 0.0859, Valid Acc: 97.35\n",
            "Epoch: 3, Train loss: 0.0213, Train Acc: 99.32, Valid loss: 0.0863, Valid Acc: 97.59\n",
            "Epoch: 4, Train loss: 0.0209, Train Acc: 99.31, Valid loss: 0.0780, Valid Acc: 97.72\n",
            "\n",
            "\n",
            "Starting pruning iteration: 2\n",
            "\n",
            "Epoch: 0, Train loss: 0.0398, Train Acc: 98.60, Valid loss: 0.1054, Valid Acc: 97.03\n",
            "Epoch: 1, Train loss: 0.0237, Train Acc: 99.18, Valid loss: 0.0829, Valid Acc: 97.56\n",
            "Epoch: 2, Train loss: 0.0165, Train Acc: 99.45, Valid loss: 0.0846, Valid Acc: 98.01\n",
            "Epoch: 3, Train loss: 0.0217, Train Acc: 99.28, Valid loss: 0.0998, Valid Acc: 97.62\n",
            "Epoch: 4, Train loss: 0.0346, Train Acc: 98.77, Valid loss: 0.1163, Valid Acc: 97.04\n",
            "\n",
            "\n",
            "Starting pruning iteration: 3\n",
            "\n",
            "Epoch: 0, Train loss: 0.0166, Train Acc: 99.47, Valid loss: 0.0889, Valid Acc: 97.80\n",
            "Epoch: 1, Train loss: 0.0139, Train Acc: 99.57, Valid loss: 0.0791, Valid Acc: 97.95\n",
            "Epoch: 2, Train loss: 0.0181, Train Acc: 99.33, Valid loss: 0.0884, Valid Acc: 97.77\n",
            "Epoch: 3, Train loss: 0.0132, Train Acc: 99.56, Valid loss: 0.0932, Valid Acc: 97.90\n",
            "Epoch: 4, Train loss: 0.0209, Train Acc: 99.29, Valid loss: 0.0997, Valid Acc: 97.65\n",
            "\n",
            "\n",
            "Starting pruning iteration: 4\n",
            "\n",
            "Epoch: 0, Train loss: 0.0142, Train Acc: 99.55, Valid loss: 0.0913, Valid Acc: 97.69\n",
            "Epoch: 1, Train loss: 0.0125, Train Acc: 99.59, Valid loss: 0.0963, Valid Acc: 97.91\n",
            "Epoch: 2, Train loss: 0.0129, Train Acc: 99.59, Valid loss: 0.0938, Valid Acc: 97.88\n",
            "Epoch: 3, Train loss: 0.0105, Train Acc: 99.66, Valid loss: 0.0956, Valid Acc: 97.97\n",
            "Epoch: 4, Train loss: 0.0176, Train Acc: 99.41, Valid loss: 0.1089, Valid Acc: 97.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePjYo_Fl8PD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    \n",
        "    #from gooey import Gooey\n",
        "    #@Gooey      \n",
        "    \n",
        "    # Arguement Parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--lr\",default= 1.2e-3, type=float, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--batch_size\", default=60, type=int)\n",
        "    parser.add_argument(\"--start_iter\", default=0, type=int)\n",
        "    parser.add_argument(\"--end_iter\", default=100, type=int)\n",
        "    parser.add_argument(\"--print_freq\", default=1, type=int)\n",
        "    parser.add_argument(\"--valid_freq\", default=1, type=int)\n",
        "    parser.add_argument(\"--resume\", action=\"store_true\")\n",
        "    parser.add_argument(\"--prune_type\", default=\"lt\", type=str, help=\"lt | reinit\")\n",
        "    parser.add_argument(\"--gpu\", default=\"0\", type=str)\n",
        "    parser.add_argument(\"--dataset\", default=\"mnist\", type=str, help=\"mnist | cifar10 | fashionmnist | cifar100\")\n",
        "    parser.add_argument(\"--arch_type\", default=\"fc1\", type=str, help=\"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\")\n",
        "    parser.add_argument(\"--prune_percent\", default=10, type=int, help=\"Pruning percent\")\n",
        "    parser.add_argument(\"--prune_iterations\", default=35, type=int, help=\"Pruning iterations count\")\n",
        "\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n",
        "    \n",
        "    \n",
        "    #FIXME resample\n",
        "    resample = False\n",
        "\n",
        "    # Looping Entire process\n",
        "    #for i in range(0, 5):\n",
        "    main(args, ITE=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}