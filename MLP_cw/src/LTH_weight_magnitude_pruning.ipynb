{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ciobiisdumb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d43ee93c276426991312ae68b781c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_78908e62204b4544aae60a8445a9680c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b63cc56a5d434624a2026b9faaf4ee8b",
              "IPY_MODEL_7161675bbeed4c11babeac8c0a540f18"
            ]
          }
        },
        "78908e62204b4544aae60a8445a9680c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b63cc56a5d434624a2026b9faaf4ee8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f78752f2e8cf4b8c86df959f00d8ddf7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c69a7e7d4d8048538a725713341919d5"
          }
        },
        "7161675bbeed4c11babeac8c0a540f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_47b592eea43340a0b6c1a9d18b176cb1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:04&lt;00:00, 39890219.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c99330c50b3b4958ab3efa16660f26d6"
          }
        },
        "f78752f2e8cf4b8c86df959f00d8ddf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c69a7e7d4d8048538a725713341919d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47b592eea43340a0b6c1a9d18b176cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c99330c50b3b4958ab3efa16660f26d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKXJP7X0Ugzj",
        "colab_type": "code",
        "outputId": "4ee3cfe8-75c4-45b5-a246-3ab56654f0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0d43ee93c276426991312ae68b781c77",
            "78908e62204b4544aae60a8445a9680c",
            "b63cc56a5d434624a2026b9faaf4ee8b",
            "7161675bbeed4c11babeac8c0a540f18",
            "f78752f2e8cf4b8c86df959f00d8ddf7",
            "c69a7e7d4d8048538a725713341919d5",
            "47b592eea43340a0b6c1a9d18b176cb1",
            "c99330c50b3b4958ab3efa16660f26d6"
          ]
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import prune\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import sys\n",
        "import types\n",
        "\n",
        "# TODO:\n",
        "# - add random seed parameters for pytorch + numpy random;\n",
        "# - check if the loss and the accuracy are calculated correctly (especially the loss);\n",
        "\n",
        "# Assumptions without which the script won't work:\n",
        "# - last layer in a net must be named \"fc_final\" + it is assumed it is a Linear layer;\n",
        "# - all models are saved + loaded on GPU so run all experiments on GPU to prevent major clusterfuck;\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Useful functions.\n",
        "\n",
        "# Reinit weights + biases. Maybe modify when we don't want biases to be \n",
        "# reinit (for comparison).\n",
        "# def reinit_and_apply_mask(model, model_init):\n",
        "#     modules_names = model._modules.keys()\n",
        "#     for module_name in modules_names:\n",
        "#         if(isinstance(model._modules[module_name], torch.nn.Linear) or\n",
        "#            isinstance(model._modules[module_name], torch.nn.Conv2d)):\n",
        "#             model._modules[module_name].weight = (model_init._modules[module_name].weight.detach() * model._modules[module_name].weight_mask.detach()).requires_grad_()\n",
        "#             model._modules[module_name].bias = torch.nn.Parameter((model_init._modules[module_name].bias.detach()), requires_grad = True)\n",
        "\n",
        "# def reinit_and_apply_mask(model, model_init):\n",
        "#     with torch.no_grad():\n",
        "#         for module_name, module in model.named_modules():\n",
        "#             if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "#                 module.weight.copy_(getattr(model_init, module_name).weight.detach().clone() * module.weight_mask.detach().clone()).requires_grad_()\n",
        "#                 module.bias.copy_(torch.nn.Parameter((getattr(model_init, module_name).bias.detach().clone()), requires_grad = True))\n",
        "\n",
        "# V3\n",
        "# def reinit_and_apply_mask(model, model_init):\n",
        "#    for module_name, module in model.named_modules():\n",
        "#        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "#            with torch.no_grad():\n",
        "#                weights = torch.tensor(module.weight)\n",
        "#                module.weight *= torch.zeros_like(module.weight)\n",
        "#                module.weight += getattr(model_init, module_name).weight.clone().detach() * module.weight_mask.clone().detach()\n",
        "#                module.bias = torch.nn.Parameter(getattr(model_init, module_name).bias.clone().detach(), requires_grad = True)\n",
        "\n",
        "# V4\n",
        "# def reinit_and_apply_mask(model, model_init):\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     for module_name, module in model.named_modules():\n",
        "#         if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "#             with torch.no_grad():\n",
        "#                 module.weight = torch.nn.Parameter(torch.from_numpy(getattr(model_init, module_name).weight.cpu().numpy() * module.weight_mask.cpu().numpy()).to(device))\n",
        "#                 module.bias = torch.nn.Parameter(getattr(model_init, module_name).bias.clone().detach(), requires_grad = True)\n",
        "\n",
        "# V5\n",
        "def reinit_and_apply_mask(model, model_init):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "            with torch.no_grad():\n",
        "                #module.weight = (getattr(model_init, module_name).weight.clone().detach() * module.weight_mask.clone().detach()).to(device).detach().requires_grad_(True)\n",
        "                #module.bias = (getattr(model_init, module_name).bias.clone().detach()).to(device).detach().requires_grad_(True)\n",
        "                module.weight = torch.nn.Parameter((getattr(model_init, module_name).weight * module.weight_mask).clone().to(device).detach().requires_grad_(True))\n",
        "                module.bias = torch.nn.Parameter((getattr(model_init, module_name).bias).clone().to(device).detach().requires_grad_(True))\n",
        "\n",
        "def count_unpruned_weights(model):\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "                count += module.weight_mask.sum()\n",
        "        return count\n",
        "\n",
        "# Re-factor this to use torch.no_grad()?\n",
        "def count_all_weights(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def flip_masks(model):\n",
        "    with torch.no_grad():\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "                module.weight_mask.mul_(-1)\n",
        "                module.weight_mask.add_(1)\n",
        "\n",
        "# Is torch.no_grad() required here? Re-look at the pruning code.\n",
        "def prune_model_l1_unstructured(model):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            if name == 'fc_out':\n",
        "                print('Debug: fc_out pruned.')\n",
        "                prune.l1_unstructured(module, name='weight', amount=0.1)\n",
        "            else:\n",
        "                prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            prune.l1_unstructured(module, name='weight', amount=0.1)\n",
        "\n",
        "def apply_prune_mask(net, keep_masks):\n",
        "\n",
        "    # Before I can zip() layers and pruning masks I need to make sure they match\n",
        "    # one-to-one by removing all the irrelevant modules:\n",
        "    prunable_layers = filter(\n",
        "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
        "            layer, nn.Linear), net.modules())\n",
        "\n",
        "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
        "        assert (layer.weight.shape == keep_mask.shape)\n",
        "\n",
        "        def hook_factory(keep_mask):\n",
        "            \"\"\"\n",
        "            The hook function can't be defined directly here because of Python's\n",
        "            late binding which would result in all hooks getting the very last\n",
        "            mask! Getting it through another function forces early binding.\n",
        "            \"\"\"\n",
        "\n",
        "            def hook(grads):\n",
        "                return grads * keep_mask\n",
        "\n",
        "            return hook\n",
        "\n",
        "        # mask[i] == 0 --> Prune parameter\n",
        "        # mask[i] == 1 --> Keep parameter\n",
        "\n",
        "        # Step 1: Set the masked weights to zero (NB the biases are ignored)\n",
        "        # Step 2: Make sure their gradients remain zero\n",
        "        layer.weight.data[keep_mask == 0.] = 0.\n",
        "        layer.weight.register_hook(hook_factory(keep_mask))\n",
        "\n",
        "# SNIP functions taken from: \n",
        "# https://github.com/mi-lad/snip/snip.py\n",
        "def snip_forward_conv2d(self, x):\n",
        "        return F.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
        "                        self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "def snip_forward_linear(self, x):\n",
        "        return F.linear(x, self.weight * self.weight_mask, self.bias)\n",
        "\n",
        "\n",
        "def SNIP(net, type_prune, keep, train_dataloader, device):\n",
        "    # TODO: shuffle?\n",
        "\n",
        "    # Grab a single batch from the training dataset\n",
        "    inputs, targets = next(iter(train_dataloader))\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Let's create a fresh copy of the network so that we're not worried about\n",
        "    # affecting the actual training-phase\n",
        "    net = copy.deepcopy(net)\n",
        "\n",
        "    # Monkey-patch the Linear and Conv2d layer to learn the multiplicative mask\n",
        "    # instead of the weights\n",
        "    for layer in net.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
        "            nn.init.xavier_normal_(layer.weight)\n",
        "            layer.weight.requires_grad = False\n",
        "\n",
        "        # Override the forward methods:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.forward = types.MethodType(snip_forward_conv2d, layer)\n",
        "\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            layer.forward = types.MethodType(snip_forward_linear, layer)\n",
        "\n",
        "    # Compute gradients (but don't apply them)\n",
        "    net.zero_grad()\n",
        "    outputs = net.forward(inputs)\n",
        "    loss = F.nll_loss(outputs, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    grads_abs = []\n",
        "    for layer in net.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            grads_abs.append(torch.abs(layer.weight_mask.grad))\n",
        "\n",
        "    # Gather all scores in a single vector and normalise\n",
        "    all_scores = torch.cat([torch.flatten(x) for x in grads_abs])\n",
        "    norm_factor = torch.sum(all_scores)\n",
        "    all_scores.div_(norm_factor)\n",
        "\n",
        "    if type_prune == 'num':\n",
        "        num_params_to_keep = keep\n",
        "    else:\n",
        "        num_params_to_keep = int(len(all_scores) * keep)\n",
        "    threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
        "    acceptable_score = threshold[-1]\n",
        "\n",
        "    keep_masks = []\n",
        "    for g in grads_abs:\n",
        "        keep_masks.append(((g / norm_factor) >= acceptable_score).float())\n",
        "\n",
        "    print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks])))\n",
        "\n",
        "    return(keep_masks)\n",
        "\n",
        "# EarlyStopping class taken from: \n",
        "# https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py.\n",
        "class EarlyStopping(object):\n",
        "    def __init__(self, mode='min', min_delta=0, patience=10, percentage='False'):\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "        \n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True\n",
        "            self.step = lambda a: False\n",
        "            \n",
        "    def step(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "        \n",
        "        if torch.isnan(metrics):\n",
        "            return True\n",
        "        \n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "            \n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('mode' + str(mode) + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best - min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - (best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best - (best * min_delta / 100)\n",
        "\n",
        "# Function for weight initialisation (taken from \n",
        "# https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch/blob/master/main.py)\n",
        "def weight_init(m):\n",
        "    '''\n",
        "    Usage:\n",
        "        model = Model()\n",
        "        model.apply(weight_init)\n",
        "    '''\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        init.normal_(m.bias.data)\n",
        "\n",
        "# Not sure if the loss calculation is correct here.\n",
        "def calculate_accuracy_and_loss(model, loader, criterion):\n",
        "    # Put the model in evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    nr_batches = len(loader)\n",
        "    total_loss = 0\n",
        "    accuracy = 0\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, targets) in enumerate(loader):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            output = model(imgs)\n",
        "\n",
        "            total_loss += (1/nr_batches) * criterion(output, targets)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += targets.shape[0]\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = correct/total * 100\n",
        "    return accuracy, total_loss\n",
        "\n",
        "def get_data_loaders(dataset):\n",
        "    # TODO: Add more dataset-dependent data loaders.\n",
        "    if dataset == 'mnist':\n",
        "        # Transforms which will be applied to the data.\n",
        "        transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.1307,), (0.3081, ))\n",
        "                                        ])\n",
        "        \n",
        "        # 0.1307, 0.3081 represent the mean + std of the mnist dataset.\n",
        "        \n",
        "        # Split the train dataset into a train + valid datasets.\n",
        "        # Must set the values of the samples in each split (here, 50000, 10000).\n",
        "        dataset = datasets.MNIST(root=os.getcwd() + '/data', train=True, download=True, transform=transform)\n",
        "        train_set, valid_set = torch.utils.data.random_split(dataset, [55000, 5000])\n",
        "\n",
        "        # Load the test dataset.\n",
        "        test_set = datasets.MNIST(root=os.getcwd() + '/data', train=False, transform=transform)\n",
        "\n",
        "    elif dataset == 'cifar10':\n",
        "        transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "                                        ])\n",
        "        dataset = datasets.CIFAR10(root=os.getcwd() + '/data', train=True, download=True, transform=transform)\n",
        "        train_set, valid_set = torch.utils.data.random_split(dataset, [45000, 5000])\n",
        "        test_set = datasets.CIFAR10(root=os.getcwd() + '/data', train=False, transform=transform)\n",
        "    \n",
        "    # Transformations will not be applied until you call a DataLoader on it.\n",
        "    # make valid_loader = test_loader if the option for the split is 0.\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "  \n",
        "def init_optimizer(optimizer_name, model, kwargs):\n",
        "    # parameters = []\n",
        "    # for module_name, module in model.named_modules():\n",
        "    #     if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "    #         parameters.append(module.weight)\n",
        "    #         parameters.append(module.bias)\n",
        "    params = []\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "            params.append(module.weight)\n",
        "            params.append(module.bias)\n",
        "    if optimizer_name == 'adam':\n",
        "        return torch.optim.Adam(params, **kwargs)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        return torch.optim.SGD(params, **kwargs)\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "# Our neural nets architectures.\n",
        "class LeNet300_100(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet300_100, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.fc2 = nn.Linear(300, 100)\n",
        "        self.fc_out = nn.Linear(100, 10)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        x = input.flatten(start_dim=1, end_dim=-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "    \n",
        "class Conv_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_2, self).__init__()\n",
        "        self.conv11 = nn.Conv2d(3, 64, 3)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(14*14*64, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc_out = nn.Linear(256, 10)\n",
        "    \n",
        "    def forward(self, input_x):\n",
        "        x = F.relu(self.conv11(input_x))\n",
        "        x = F.relu(self.conv12(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 14*14*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "class Conv_4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_4, self).__init__()\n",
        "        self.conv11 = nn.Conv2d(3, 64, 3)\n",
        "        self.conv12 = nn.Conv2d(64, 64, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv21 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv22 = nn.Conv2d(128, 128, 3)\n",
        "        self.fc1 = nn.Linear(5*5*128, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc_out = nn.Linear(256, 10)\n",
        "    \n",
        "    def forward(self, input_x):\n",
        "        x = F.relu(self.conv11(input_x))\n",
        "        x = F.relu(self.conv12(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv21(x))\n",
        "        x = F.relu(self.conv22(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 5*5*128)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.f_out(x)\n",
        "        return x\n",
        "# ------------------------------------------------------------------------------\n",
        "# Argument parser:\n",
        "# parser = argparse.ArgumentParser()\n",
        "\n",
        "# # General arguments:\n",
        "# parser.add_argument(\"--experiment_name\", type=str)\n",
        "# parser.add_argument(\"--learning_rate\", default=0, type=float, help=\"LTH fc 1.2e-3, conv2 2e-4, conv4 3e-4\")\n",
        "# parser.add_argument(\"--batch_size\", default=60, type=int)\n",
        "# parser.add_argument(\"--eval_freq\", default=100, type=int)\n",
        "# parser.add_argument(\"--max_nr_epochs\", default=50, type=int, help=\"Maximum number of epochs\")\n",
        "# parser.add_argument(\"--model\", help=\"fc | conv2 | conv4\")\n",
        "# parser.add_argument(\"--dataset\", type=str, help=\"mnist | cifar10\")\n",
        "# parser.add_argument(\"--experiment_type\", type=str, default=\"LTH\", help=\"LTH | SNIP | LTH_flip\")\n",
        "# parser.add_argument(\"--optimizer\", type=str, default=\"adam\", help=\"adam | sgd\")\n",
        "# parser.add_argument(\"--weight_decay\", type=float, default=0)\n",
        "# parser.add_argument(\"--patience\", type=float, default=10, help=\"Roughly 1 epoch. patience*eval_freq = number of iterations we have patience for. An epoch has training_samples_nr/batch_size iterations.\")\n",
        "\n",
        "# # Method specific arguments:\n",
        "# parser.add_argument(\"--prune_iterations\", default=20, type=int)\n",
        "# parser.add_argument(\"--pruning_criterion\", type=str, default=\"weight_magnitude\", help=\"Pruning criterion, available only for LTH (so far)\")\n",
        "\n",
        "# # Arguments for SGD.\n",
        "# parser.add_argument(\"--momentum\", type=float, default=0)\n",
        "\n",
        "# # Argument used only for the LTH_flip experiment.\n",
        "# parser.add_argument(\"--nr_lottery_tickets\", type=int, default=2)\n",
        "\n",
        "# #parser.add_argument(\"--prune_type\", default=\"lt\", type=str, help=\"lt | reinit\")\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# #-------------------------------------------------------------------------------\n",
        "# # All the variables + initialisations we need for an experiment.\n",
        "# learning_rate = args.learning_rate\n",
        "# batch_size = args.batch_size\n",
        "# eval_freq = args.eval_freq\n",
        "# experiment_name = args.experiment_name\n",
        "# epochs = args.max_nr_epochs\n",
        "# prune_iterations = args.prune_iterations\n",
        "# model_name = args.model\n",
        "# experiment_type = args.experiment_type\n",
        "# pruning_criterion = args.pruning_criterion\n",
        "\n",
        "# # Optimizer params.\n",
        "# optimizer_name = args.optimizer\n",
        "# weight_decay = args.weight_decay\n",
        "# momentum = args.momentum\n",
        "\n",
        "# # LTH_flip experiment param.\n",
        "# nr_lottery_tickets = args.nr_lottery_tickets\n",
        "\n",
        "learning_rate = 0.0002\n",
        "batch_size = 60\n",
        "eval_freq = 500\n",
        "experiment_name = 'debugging_conv_2'\n",
        "epochs = 2\n",
        "prune_iterations = 3\n",
        "model_name = 'conv2'\n",
        "experiment_type = 'LTH'\n",
        "pruning_criterion = 'weight_magnitude'\n",
        "optimizer_name = 'adam'\n",
        "weight_decay = 0\n",
        "momentum = 0\n",
        "patience = 3\n",
        "nr_lottery_tickets = 2\n",
        "\n",
        "# Put the optimizer params in a dictionary (will be passed to init_optimizer).\n",
        "optimizer_args = dict()\n",
        "optimizer_args[\"lr\"] = learning_rate\n",
        "optimizer_args[\"weight_decay\"] = weight_decay\n",
        "if optimizer_name == \"sgd\":\n",
        "    optimizer_args['momentum'] = momentum\t\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Our experiments are hardcoded to use certain datasets:\n",
        "# FC -> MNIST, Conv-2, Conv-4 -> CIFAR10.\n",
        "if model_name == \"fc\":\n",
        "    model = LeNet300_100().to(device)\n",
        "    dataset = \"mnist\"\n",
        "elif model_name == \"conv2\":\n",
        "    model = Conv_2().to(device)\n",
        "    dataset = \"cifar10\"\n",
        "elif model_name == \"conv4\":\n",
        "    model = Conv_4().to(device)\n",
        "    dataset = \"cifar10\"\n",
        "else:\n",
        "    print(\"Invalid model.\")\n",
        "    sys.exit()\n",
        "  \n",
        "# Get the data loaders for MNIST/CIFAR10.\n",
        "train_loader, valid_loader, test_loader = get_data_loaders(dataset)\n",
        "\n",
        "# If the default value of 0 was given to learning_rate, give it the appropriate\n",
        "# value instead.\n",
        "if learning_rate == 0:\n",
        "    if model_name == \"fc\":\n",
        "        learning_rate = 0.0012\n",
        "    elif model_name == \"conv2\":\n",
        "        learning_rate = 0.0002\n",
        "    elif model_name == \"conv4\":\n",
        "        learning_rate = 0.0003\n",
        "    else:\n",
        "        print(\"Invalid model.\")\n",
        "        sys.exit()\n",
        "\n",
        "# Initialise weights.\n",
        "model.apply(weight_init)\n",
        "\n",
        "# Save initial model for reference.\n",
        "os.makedirs(os.getcwd() + '/' + experiment_name, exist_ok=True)\n",
        "torch.save(model, os.getcwd() + '/' + experiment_name + '/model_init.pth')\n",
        "\n",
        "# Weight decay? - SNIP does have it with SGD, doesn't have it with SGD. \n",
        "#               - LTH seems to have some experiments with weight decay, and some without + unspecified value.\n",
        "optimizer = init_optimizer(optimizer_name, model, optimizer_args)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "writer = SummaryWriter()\n",
        "os.makedirs(os.getcwd() + '/' + experiment_name + '/stats', exist_ok=True)\n",
        "writer = SummaryWriter(os.getcwd() + '/' + experiment_name + '/stats')\n",
        "    \n",
        "def train_SNIP():\n",
        "    prune_iterations = 1\n",
        "    global optimizer\n",
        "    \n",
        "    best_train_accs = np.zeros((prune_iterations, ))\n",
        "    best_valid_accs = np.zeros((prune_iterations, ))\n",
        "    best_test_accs = np.zeros((prune_iterations, ))\n",
        "    best_train_losses = np.zeros((prune_iterations, ))\n",
        "    best_valid_losses = np.zeros((prune_iterations, ))\n",
        "    best_test_losses = np.zeros((prune_iterations, ))\n",
        "    early_stop_iterations = np.zeros((prune_iterations, ))\n",
        "    unpruned_weights_counts = np.zeros((prune_iterations, ))\n",
        "    \n",
        "    experiment_PATH = os.getcwd() + '/' + experiment_name\n",
        "\n",
        "    # Re-init iteration number.\n",
        "    iteration_nr = 0\n",
        "        \n",
        "    # Re-init best validation acc.\n",
        "    best_valid_acc = 0\n",
        "\n",
        "    unpruned_weights_counts[0] = count_all_weights(model)\n",
        "    \n",
        "    keep_masks = SNIP(model, 'perc', 0.05, train_loader, device)\n",
        "    apply_prune_mask(model, keep_masks)\n",
        "    \n",
        "    # Initializing EarlyStopping\n",
        "    es = EarlyStopping(patience=patience)\n",
        "    \n",
        "    train_acc, train_loss = calculate_accuracy_and_loss(model, train_loader, criterion)\n",
        "    valid_acc, valid_loss = calculate_accuracy_and_loss(model, valid_loader, criterion)\n",
        "    test_acc, test_loss = calculate_accuracy_and_loss(model, test_loader, criterion)\n",
        "\n",
        "    writer.add_scalar('Accuracy/train_' + str(prune_iteration), train_acc, iteration_nr)\n",
        "    writer.add_scalar('Loss/train_' + str(prune_iteration), train_loss, iteration_nr)\n",
        "    writer.add_scalar('Accuracy/valid_' + str(prune_iteration), valid_acc, iteration_nr)\n",
        "    writer.add_scalar('Loss/valid_' + str(prune_iteration), valid_loss, iteration_nr)\n",
        "    writer.add_scalar('Accuracy/test_' + str(prune_iteration), test_acc, iteration_nr)\n",
        "    writer.add_scalar('Loss/test_' + str(prune_iteration), test_loss, iteration_nr)\n",
        "\n",
        "    print('Stats before training: Train loss: {:.4f}, Train Acc: {:.2f}, Valid loss: {:.4f}, Valid Acc: {:.2f}'.format(train_loss, train_acc, valid_loss, valid_acc)) \n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
        "\n",
        "            iteration_nr += 1\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "            output = model(imgs)\n",
        "                    \n",
        "            train_loss = criterion(output, targets)\n",
        "            train_loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration_nr % eval_freq == 0:\n",
        "                train_acc, train_loss = calculate_accuracy_and_loss(model, train_loader, criterion)\n",
        "                valid_acc, valid_loss = calculate_accuracy_and_loss(model, valid_loader, criterion)\n",
        "                test_acc, test_loss = calculate_accuracy_and_loss(model, test_loader, criterion)\n",
        "\n",
        "                writer.add_scalar('Accuracy/train_' + str(0), train_acc, iteration_nr)\n",
        "                writer.add_scalar('Loss/train_' + str(0), train_loss, iteration_nr)\n",
        "                writer.add_scalar('Accuracy/valid_' + str(0), valid_acc, iteration_nr)\n",
        "                writer.add_scalar('Loss/valid_' + str(0), valid_loss, iteration_nr)\n",
        "                writer.add_scalar('Accuracy/test_' + str(0), test_acc, iteration_nr)\n",
        "                writer.add_scalar('Loss/test_' + str(0), test_loss, iteration_nr)\n",
        "\n",
        "                # Maybe save the best models here, now we're interested only in best accs/losses.\n",
        "                if(valid_acc > best_valid_acc):\n",
        "                    best_train_accs[0] = train_acc\n",
        "                    best_train_losses[0] = train_loss\n",
        "                    best_valid_accs[0] = valid_acc\n",
        "                    best_valid_losses[0] = valid_loss\n",
        "                    best_test_accs[0] = test_acc\n",
        "                    best_test_losses[0] = test_loss\n",
        "                    early_stop_iterations[0] = iteration_nr\n",
        "                            \n",
        "                    best_valid_acc = valid_acc\n",
        "                            \n",
        "                    torch.save(model, experiment_PATH + '/' + 'best_model_prune_iteration_' + str(0) + '.pth')\n",
        "\n",
        "                if es.step(valid_loss):\n",
        "                    break\n",
        "\n",
        "        print('Epoch: ' + str(epoch) +  ', Train loss: {:.4f}, Train Acc: {:.2f}, Valid loss: {:.4f}, Valid Acc: {:.2f}'.format(train_loss, train_acc, valid_loss, valid_acc))\n",
        "    \n",
        "def train_LTH():\n",
        "    global optimizer\n",
        "    global prune_iterations\n",
        "    global model\n",
        "    prune_iterations += 1\n",
        "\n",
        "    best_train_accs = np.zeros((prune_iterations, ))\n",
        "    best_valid_accs = np.zeros((prune_iterations, ))\n",
        "    best_test_accs = np.zeros((prune_iterations, ))\n",
        "    best_train_losses = np.zeros((prune_iterations, ))\n",
        "    best_valid_losses = np.zeros((prune_iterations, ))\n",
        "    best_test_losses = np.zeros((prune_iterations, ))\n",
        "    early_stop_iterations = np.zeros((prune_iterations, ))\n",
        "    unpruned_weights_counts = np.zeros((prune_iterations, ))\n",
        "\n",
        "    experiment_PATH = os.getcwd() + '/' + experiment_name\n",
        "\n",
        "    best_valid_acc = 0\n",
        "    unpruned_weights_counts[0] = count_all_weights(model)\n",
        "    for prune_iteration in range(prune_iterations):\n",
        "        print('\\n\\nStarting prune iteration: ' + str(prune_iteration) + '\\n')\n",
        "\n",
        "        print('DEBUG: Weights BEFORE pruning')\n",
        "        print(model.fc1.weight)\n",
        "\n",
        "        if prune_iteration != 0:\n",
        "            # do the pruning here.\n",
        "            # Before pruning (or after, we can save the mask here, if interested).\n",
        "            prune_model(model)\n",
        "            \n",
        "            # Count number of unpruned weights + add to array.\n",
        "            unpruned_weights_counts[prune_iteration] = count_unpruned_weights(model)\n",
        "            \n",
        "            model_init = torch.load(experiment_PATH + '/model_init.pth').to(device)\n",
        "            # with torch.no_grad():\n",
        "            #     model_init.fc1.weight *= model.fc1.weight_mask.detach().clone()\n",
        "            #     model_init.fc2.weight *= model.fc2.weight_mask.detach().clone()\n",
        "            #     model_init.fc_out.weight *= model.fc_out.weight_mask.detach().clone()\n",
        "            # model = model_init\n",
        "            reinit_and_apply_mask(model, model_init)\n",
        "            del model_init\n",
        "            \n",
        "            # Re-initialize the optimizer.\n",
        "            optimizer = init_optimizer(optimizer_name, model, optimizer_args)\n",
        "\n",
        "        print('DEBUG: Weights AFTER pruning')\n",
        "        print(model.fc1.weight)\n",
        "        \n",
        "        # Re-init iteration number.\n",
        "        iteration_nr = 0\n",
        "        \n",
        "        # Re-init best validation acc.\n",
        "        best_valid_acc = 0\n",
        "        \n",
        "        # Initializing EarlyStopping\n",
        "        es = EarlyStopping(patience=patience)\n",
        "\n",
        "        train_acc, train_loss = calculate_accuracy_and_loss(model, train_loader, criterion)\n",
        "        valid_acc, valid_loss = calculate_accuracy_and_loss(model, valid_loader, criterion)\n",
        "        test_acc, test_loss = calculate_accuracy_and_loss(model, test_loader, criterion)\n",
        "\n",
        "        writer.add_scalar('Accuracy/train_' + str(prune_iteration), train_acc, iteration_nr)\n",
        "        writer.add_scalar('Loss/train_' + str(prune_iteration), train_loss, iteration_nr)\n",
        "        writer.add_scalar('Accuracy/valid_' + str(prune_iteration), valid_acc, iteration_nr)\n",
        "        writer.add_scalar('Loss/valid_' + str(prune_iteration), valid_loss, iteration_nr)\n",
        "        writer.add_scalar('Accuracy/test_' + str(prune_iteration), test_acc, iteration_nr)\n",
        "        writer.add_scalar('Loss/test_' + str(prune_iteration), test_loss, iteration_nr)\n",
        "\n",
        "        print('Stats before training: Train loss: {:.4f}, Train Acc: {:.2f}, Valid loss: {:.4f}, Valid Acc: {:.2f}'.format(train_loss, train_acc, valid_loss, valid_acc)) \n",
        "\n",
        "        for epoch in range(epochs):\n",
        "                model.train()\n",
        "                for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
        "\n",
        "                    iteration_nr += 1\n",
        "                                        \n",
        "                    optimizer.zero_grad()\n",
        "                    imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "                    output = model(imgs)\n",
        "                    \n",
        "                    train_loss = criterion(output, targets)\n",
        "                    train_loss.backward()\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if iteration_nr % eval_freq == 0:\n",
        "                        train_acc, train_loss = calculate_accuracy_and_loss(model, train_loader, criterion)\n",
        "                        valid_acc, valid_loss = calculate_accuracy_and_loss(model, valid_loader, criterion)\n",
        "                        test_acc, test_loss = calculate_accuracy_and_loss(model, test_loader, criterion)                                                      \n",
        "\n",
        "                        writer.add_scalar('Accuracy/train_' + str(prune_iteration), train_acc, iteration_nr)\n",
        "                        writer.add_scalar('Loss/train_' + str(prune_iteration), train_loss, iteration_nr)\n",
        "                        writer.add_scalar('Accuracy/valid_' + str(prune_iteration), valid_acc, iteration_nr)\n",
        "                        writer.add_scalar('Loss/valid_' + str(prune_iteration), valid_loss, iteration_nr)\n",
        "                        writer.add_scalar('Accuracy/test_' + str(prune_iteration), test_acc, iteration_nr)\n",
        "                        writer.add_scalar('Loss/test_' + str(prune_iteration), test_loss, iteration_nr)\n",
        "\n",
        "                        # Maybe save the best models here, now we're interested only in best accs/losses.\n",
        "                        if(valid_acc > best_valid_acc):\n",
        "                            best_train_accs[prune_iteration] = train_acc\n",
        "                            best_train_losses[prune_iteration] = train_loss\n",
        "                            best_valid_accs[prune_iteration] = valid_acc\n",
        "                            best_valid_losses[prune_iteration] = valid_loss\n",
        "                            best_test_accs[prune_iteration] = test_acc\n",
        "                            best_test_losses[prune_iteration] = test_loss\n",
        "                            early_stop_iterations[prune_iteration] = iteration_nr\n",
        "                            \n",
        "                            best_valid_acc = valid_acc\n",
        "                            \n",
        "                            torch.save(model, experiment_PATH + '/' + 'best_model_prune_iteration_' + str(prune_iteration) + '.pth')\n",
        "\n",
        "                        if es.step(valid_loss):\n",
        "                            break\n",
        "\n",
        "                print('Epoch: ' + str(epoch) +  ', Train loss: {:.4f}, Train Acc: {:.2f}, Valid loss: {:.4f}, Valid Acc: {:.2f}'.format(train_loss, train_acc, valid_loss, valid_acc))\n",
        "        \n",
        "        \n",
        "    np.save(experiment_PATH + '/best_train_accs.npy', best_train_accs)\n",
        "    np.save(experiment_PATH + '/best_train_losses.npy', best_train_losses)\n",
        "    np.save(experiment_PATH + '/best_valid_accs.npy', best_valid_accs)\n",
        "    np.save(experiment_PATH + '/best_valid_losses.npy', best_valid_losses)\n",
        "    np.save(experiment_PATH + '/best_test_accs.npy', best_test_accs)\n",
        "    np.save(experiment_PATH + '/best_test_losses.npy', best_test_losses)\n",
        "    np.save(experiment_PATH + '/early_stop_iterations.npy', early_stop_iterations)\n",
        "    np.save(experiment_PATH + '/unpruned_weights_counts.npy', unpruned_weights_counts)\n",
        "\n",
        "def train_LTH_flip():\n",
        "    global optimizer\n",
        "    global prune_iterations\n",
        "    global model\n",
        "    prune_iterations += 1\n",
        "    \n",
        "    for ticket_nr in range(nr_lottery_tickets):\t\n",
        "        print(\"\\nFinding lottery ticket number \" + str(ticket_nr+1))\n",
        "\n",
        "        best_train_accs = np.zeros((prune_iterations, ))\n",
        "        best_valid_accs = np.zeros((prune_iterations, ))\n",
        "        best_test_accs = np.zeros((prune_iterations, ))\n",
        "        best_train_losses = np.zeros((prune_iterations, ))\n",
        "        best_valid_losses = np.zeros((prune_iterations, ))\n",
        "        best_test_losses = np.zeros((prune_iterations, ))\n",
        "        early_stop_iterations = np.zeros((prune_iterations, ))\n",
        "        unpruned_weights_counts = np.zeros((prune_iterations, ))\n",
        "\n",
        "        experiment_PATH = os.getcwd() + '/' + experiment_name + '/lottery_ticket_nr_' + str(ticket_nr)\n",
        "        os.makedirs(experiment_PATH, exist_ok=True)\n",
        "        \n",
        "        writer = SummaryWriter()\n",
        "        os.makedirs(experiment_PATH + '/stats', exist_ok=True)\n",
        "        writer = SummaryWriter(experiment_PATH + '/stats')\n",
        "\n",
        "        best_valid_acc = 0\n",
        "        unpruned_weights_counts[0] = count_all_weights(model)\n",
        "        \n",
        "        if ticket_nr >= 1:\n",
        "            if ticket_nr == 1:\n",
        "                model = torch.load(os.getcwd() + '/' + experiment_name + '/lottery_ticket_nr_0/best_model_prune_iteration_' + str(prune_iterations-1) + '.pth').to(device)\n",
        "            elif ticket_nr > 1:\n",
        "                model = torch.load(os.getcwd() + '/' + experiment_name + '/lottery_ticket_nr_0/best_model_prune_iteration_' + str(prune_iterations-1) + '.pth').to(device)\n",
        "                for ticket in range(1, ticket_nr):\n",
        "                    model_aux = torch.load(os.getcwd() + '/' + experiment_name + '/lottery_ticket_nr_' + str(ticket) + '/best_model_prune_iteration_' + str(prune_iterations-1) + '.pth').to(device)\n",
        "                    add_masks(model, model_aux)\n",
        "                    del model_aux\n",
        "            flip_masks(model)\n",
        "            \n",
        "            model_init = torch.load(os.getcwd() + '/' + experiment_name + '/model_init.pth').to(device)\n",
        "            reinit_and_apply_mask(model, model_init)\n",
        "            del model_init\n",
        "            optimizer = init_optimizer(optimizer_name, model, optimizer_args)\n",
        "\n",
        "        for prune_iteration in range(prune_iterations):\n",
        "            print('\\n\\nStarting prune iteration: ' + str(prune_iteration) + '\\n')\n",
        "\n",
        "            if prune_iteration != 0:\n",
        "                # do the pruning here.\n",
        "                # Before pruning (or after, we can save the mask here, if interested).\n",
        "                prune_model(model)\n",
        "\n",
        "                # Count number of unpruned weights + add to array.\n",
        "                unpruned_weights_counts[prune_iteration] = count_unpruned_weights(model)\n",
        "\n",
        "                model_init = torch.load(os.getcwd() + '/' + experiment_name + '/model_init.pth').to(device)\n",
        "\n",
        "                reinit_and_apply_mask(model, model_init)\n",
        "                del model_init\n",
        "\n",
        "                # Re-initialize the optimizer.\n",
        "                optimizer = init_optimizer(optimizer_name, model, optimizer_args)\n",
        "\n",
        "            # Re-init iteration number.\n",
        "            iteration_nr = 0\n",
        "            \n",
        "            # Initializing EarlyStopping\n",
        "            es = EarlyStopping(patience=patience)\n",
        "\n",
        "            train_acc, train_loss = calculate_accuracy_and_loss(model, train_loader, criterion)\n",
        "            valid_acc, valid_loss = calculate_accuracy_and_loss(model, valid_loader, criterion)\n",
        "            test_acc, test_loss = calculate_accuracy_and_loss(model, test_loader, criterion)\n",
        "\n",
        "            writer.add_scalar('Accuracy/train_' + str(prune_iteration), train_acc, iteration_nr)\n",
        "            writer.add_scalar('Loss/train_' + str(prune_iteration), train_loss, iteration_nr)\n",
        "            writer.add_scalar('Accuracy/valid_' + str(prune_iteration), valid_acc, iteration_nr)\n",
        "            writer.add_scalar('Loss/valid_' + str(prune_iteration), valid_loss, iteration_nr)\n",
        "            writer.add_scalar('Accuracy/test_' + str(prune_iteration), test_acc, iteration_nr)\n",
        "            writer.add_scalar('Loss/test_' + str(prune_iteration), test_loss, iteration_nr)\n",
        "\n",
        "            print('Stats before training: Train loss: {:.4f}, Train Acc: {:.2f}, Valid loss: {:.4f}, Valid Acc: {:.2f}'.format(train_loss, train_acc, valid_loss, valid_acc)) \n",
        "\n",
        "            # Re-init best validation acc.\n",
        "            best_valid_acc = 0\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
        "                    iteration += 1\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "                    output = model(imgs)\n",
        "\n",
        "                    train_loss = criterion(output, targets)\n",
        "                    train_loss.backward()\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if iteration_nr % eval_freq == 0:\n",
        "                        train_acc, train_loss = calculate_accuracy_and_loss(model, train_loader, criterion)\n",
        "                        valid_acc, valid_loss = calculate_accuracy_and_loss(model, valid_loader, criterion)\n",
        "                        test_acc, test_loss = calculate_accuracy_and_loss(model, test_loader, criterion)\n",
        "\n",
        "                        writer.add_scalar('Accuracy/train_' + str(prune_iteration), train_acc, iteration_nr)\n",
        "                        writer.add_scalar('Loss/train_' + str(prune_iteration), train_loss, iteration_nr)\n",
        "                        writer.add_scalar('Accuracy/valid_' + str(prune_iteration), valid_acc, iteration_nr)\n",
        "                        writer.add_scalar('Loss/valid_' + str(prune_iteration), valid_loss, iteration_nr)\n",
        "                        writer.add_scalar('Accuracy/test_' + str(prune_iteration), test_acc, iteration_nr)\n",
        "                        writer.add_scalar('Loss/test_' + str(prune_iteration), test_loss, iteration_nr)\n",
        "\n",
        "                        # Maybe save the best models here, now we're interested only in best accs/losses.\n",
        "                        if(valid_acc > best_valid_acc):\n",
        "                            best_train_accs[prune_iteration] = train_acc\n",
        "                            best_train_losses[prune_iteration] = train_loss\n",
        "                            best_valid_accs[prune_iteration] = valid_acc\n",
        "                            best_valid_losses[prune_iteration] = valid_loss\n",
        "                            best_test_accs[prune_iteration] = test_acc\n",
        "                            best_test_losses[prune_iteration] = test_loss\n",
        "                            early_stop_iterations[prune_iteration] = iteration_nr\n",
        "\n",
        "                            best_valid_acc = valid_acc\n",
        "\n",
        "                            torch.save(model, experiment_PATH + '/' + 'best_model_prune_iteration_' + str(prune_iteration) + '.pth')\n",
        "                            \n",
        "                        if es.step(valid_loss):\n",
        "                            break\n",
        "\n",
        "                print('Epoch: ' + str(epoch) +  ', Train loss: {:.4f}, Train Acc: {:.2f}, Valid loss: {:.4f}, Valid Acc: {:.2f}'.format(train_loss, train_acc, valid_loss, valid_acc))\n",
        "\n",
        "\n",
        "        np.save(experiment_PATH + '/best_train_accs.n   py', best_train_accs)\n",
        "        np.save(experiment_PATH + '/best_train_losses.npy', best_train_losses)\n",
        "        np.save(experiment_PATH + '/best_valid_accs.npy', best_valid_accs)\n",
        "        np.save(experiment_PATH + '/best_valid_losses.npy', best_valid_losses)\n",
        "        np.save(experiment_PATH + '/best_test_accs.npy', best_test_accs)\n",
        "        np.save(experiment_PATH + '/best_test_losses.npy', best_test_losses)\n",
        "        np.save(experiment_PATH + '/early_stop_iterations.npy', early_stop_iterations)\n",
        "        np.save(experiment_PATH + '/unpruned_weights_counts.npy', unpruned_weights_counts)\n",
        "\n",
        "# # Setup all variables needed in the train function before calling it.\n",
        "if experiment_type == \"LTH\":\n",
        "    if pruning_criterion == \"weight_magnitude\":\n",
        "        prune_model = prune_model_l1_unstructured\n",
        "    else:\n",
        "        print(\"Invalid pruning criterion.\")\n",
        "        sys.exit()    \n",
        "    train_LTH()\n",
        "elif experiment_type == \"SNIP\":\n",
        "    train_SNIP()\n",
        "elif experiment_type == \"LTH_flip\":\n",
        "    if pruning_criterion == \"weight_magnitude\":\n",
        "        prune_model = prune_model_l1_unstructured\n",
        "    else:\n",
        "        print(\"Invalid pruning criterion.\")\n",
        "        sys.exit()    \n",
        "    train_LTH_flip()\n",
        "else:\n",
        "    print(\"Invalid pruning strategy.\")\n",
        "    sys.exit()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d43ee93c276426991312ae68b781c77",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/data/cifar-10-python.tar.gz to /content/data\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Conv_2. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Starting prune iteration: 0\n",
            "\n",
            "DEBUG: Weights BEFORE pruning\n",
            "Parameter containing:\n",
            "tensor([[-0.0099,  0.0087,  0.0163,  ...,  0.0104,  0.0065,  0.0078],\n",
            "        [-0.0090, -0.0236, -0.0099,  ..., -0.0046, -0.0125, -0.0079],\n",
            "        [ 0.0146,  0.0341, -0.0026,  ..., -0.0102,  0.0031, -0.0159],\n",
            "        ...,\n",
            "        [-0.0023, -0.0145,  0.0031,  ..., -0.0074,  0.0319, -0.0088],\n",
            "        [ 0.0080, -0.0256,  0.0056,  ...,  0.0076,  0.0017, -0.0083],\n",
            "        [ 0.0158,  0.0017,  0.0184,  ..., -0.0084,  0.0020, -0.0115]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "DEBUG: Weights AFTER pruning\n",
            "Parameter containing:\n",
            "tensor([[-0.0099,  0.0087,  0.0163,  ...,  0.0104,  0.0065,  0.0078],\n",
            "        [-0.0090, -0.0236, -0.0099,  ..., -0.0046, -0.0125, -0.0079],\n",
            "        [ 0.0146,  0.0341, -0.0026,  ..., -0.0102,  0.0031, -0.0159],\n",
            "        ...,\n",
            "        [-0.0023, -0.0145,  0.0031,  ..., -0.0074,  0.0319, -0.0088],\n",
            "        [ 0.0080, -0.0256,  0.0056,  ...,  0.0076,  0.0017, -0.0083],\n",
            "        [ 0.0158,  0.0017,  0.0184,  ..., -0.0084,  0.0020, -0.0115]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Stats before training: Train loss: 3.6851, Train Acc: 9.96, Valid loss: 3.6661, Valid Acc: 10.38\n",
            "Epoch: 0, Train loss: 1.3637, Train Acc: 47.62, Valid loss: 1.4432, Valid Acc: 47.31\n",
            "Epoch: 1, Train loss: 1.0781, Train Acc: 62.49, Valid loss: 1.1657, Valid Acc: 58.53\n",
            "\n",
            "\n",
            "Starting prune iteration: 1\n",
            "\n",
            "DEBUG: Weights BEFORE pruning\n",
            "Parameter containing:\n",
            "tensor([[-1.1626e-02,  7.0908e-03,  1.4239e-02,  ...,  8.4629e-03,\n",
            "          4.9597e-03,  5.9915e-03],\n",
            "        [-1.0096e-02, -2.4708e-02, -1.1062e-02,  ..., -4.5963e-03,\n",
            "         -1.3671e-02, -7.9090e-03],\n",
            "        [ 7.7215e-03,  2.7618e-02, -9.0479e-03,  ..., -1.2693e-02,\n",
            "          2.3140e-03, -1.7273e-02],\n",
            "        ...,\n",
            "        [-8.0600e-05, -1.4232e-02,  3.5884e-03,  ...,  5.8972e-03,\n",
            "          3.4442e-02,  1.4075e-03],\n",
            "        [ 6.7932e-03, -2.6800e-02,  4.4946e-03,  ...,  6.3605e-03,\n",
            "          5.9607e-04, -9.4613e-03],\n",
            "        [ 1.4742e-02,  6.5445e-04,  1.7363e-02,  ..., -9.6208e-03,\n",
            "          9.3370e-04, -1.2563e-02]], device='cuda:0', requires_grad=True)\n",
            "Debug: fc_out pruned.\n",
            "DEBUG: Weights AFTER pruning\n",
            "Parameter containing:\n",
            "tensor([[-0.0099,  0.0087,  0.0163,  ...,  0.0104,  0.0065,  0.0078],\n",
            "        [-0.0090, -0.0236, -0.0099,  ..., -0.0046, -0.0125, -0.0079],\n",
            "        [ 0.0146,  0.0341, -0.0026,  ..., -0.0102,  0.0000, -0.0159],\n",
            "        ...,\n",
            "        [-0.0000, -0.0145,  0.0031,  ..., -0.0074,  0.0319, -0.0000],\n",
            "        [ 0.0080, -0.0256,  0.0056,  ...,  0.0076,  0.0000, -0.0083],\n",
            "        [ 0.0158,  0.0000,  0.0184,  ..., -0.0084,  0.0000, -0.0115]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Stats before training: Train loss: 2.5143, Train Acc: 10.81, Valid loss: 2.4923, Valid Acc: 10.90\n",
            "Epoch: 0, Train loss: 1.3427, Train Acc: 53.45, Valid loss: 1.3492, Valid Acc: 52.09\n",
            "Epoch: 1, Train loss: 0.9968, Train Acc: 64.86, Valid loss: 1.1065, Valid Acc: 59.96\n",
            "\n",
            "\n",
            "Starting prune iteration: 2\n",
            "\n",
            "DEBUG: Weights BEFORE pruning\n",
            "Parameter containing:\n",
            "tensor([[-0.0127,  0.0054,  0.0125,  ...,  0.0091,  0.0038,  0.0018],\n",
            "        [-0.0122, -0.0236, -0.0099,  ..., -0.0079, -0.0158, -0.0112],\n",
            "        [ 0.0122,  0.0317, -0.0050,  ..., -0.0126,  0.0000, -0.0159],\n",
            "        ...,\n",
            "        [-0.0000, -0.0164,  0.0012,  ...,  0.0036,  0.0351,  0.0000],\n",
            "        [ 0.0151, -0.0196,  0.0088,  ...,  0.0016, -0.0000, -0.0398],\n",
            "        [ 0.0150, -0.0000,  0.0175,  ..., -0.0110, -0.0000, -0.0138]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Debug: fc_out pruned.\n",
            "DEBUG: Weights AFTER pruning\n",
            "Parameter containing:\n",
            "tensor([[-0.0099,  0.0000,  0.0163,  ...,  0.0104,  0.0000,  0.0000],\n",
            "        [-0.0090, -0.0236, -0.0099,  ..., -0.0046, -0.0125, -0.0079],\n",
            "        [ 0.0146,  0.0341, -0.0000,  ..., -0.0102,  0.0000, -0.0159],\n",
            "        ...,\n",
            "        [-0.0000, -0.0145,  0.0000,  ..., -0.0000,  0.0319, -0.0000],\n",
            "        [ 0.0080, -0.0256,  0.0056,  ...,  0.0000,  0.0000, -0.0083],\n",
            "        [ 0.0158,  0.0000,  0.0184,  ..., -0.0084,  0.0000, -0.0115]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Stats before training: Train loss: 2.6674, Train Acc: 10.02, Valid loss: 2.6274, Valid Acc: 10.22\n",
            "Epoch: 0, Train loss: 1.0060, Train Acc: 57.26, Valid loss: 1.2510, Valid Acc: 55.08\n",
            "Epoch: 1, Train loss: 0.9653, Train Acc: 66.12, Valid loss: 1.0851, Valid Acc: 62.37\n",
            "\n",
            "\n",
            "Starting prune iteration: 3\n",
            "\n",
            "DEBUG: Weights BEFORE pruning\n",
            "Parameter containing:\n",
            "tensor([[-0.0139,  0.0000,  0.0092,  ...,  0.0096, -0.0000, -0.0000],\n",
            "        [-0.0090, -0.0236, -0.0099,  ..., -0.0046, -0.0125, -0.0079],\n",
            "        [ 0.0146,  0.0341, -0.0000,  ..., -0.0119, -0.0000, -0.0176],\n",
            "        ...,\n",
            "        [ 0.0000, -0.0174,  0.0000,  ...,  0.0000,  0.0322,  0.0000],\n",
            "        [ 0.0111, -0.0249,  0.0065,  ...,  0.0000, -0.0000, -0.0528],\n",
            "        [ 0.0148, -0.0000,  0.0174,  ..., -0.0099, -0.0000, -0.0128]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Debug: fc_out pruned.\n",
            "DEBUG: Weights AFTER pruning\n",
            "Parameter containing:\n",
            "tensor([[-0.0099,  0.0000,  0.0163,  ...,  0.0104,  0.0000,  0.0000],\n",
            "        [-0.0090, -0.0236, -0.0099,  ..., -0.0000, -0.0125, -0.0079],\n",
            "        [ 0.0146,  0.0341, -0.0000,  ..., -0.0102,  0.0000, -0.0159],\n",
            "        ...,\n",
            "        [-0.0000, -0.0145,  0.0000,  ..., -0.0000,  0.0319, -0.0000],\n",
            "        [ 0.0080, -0.0256,  0.0000,  ...,  0.0000,  0.0000, -0.0083],\n",
            "        [ 0.0158,  0.0000,  0.0184,  ..., -0.0084,  0.0000, -0.0115]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "Stats before training: Train loss: 2.5691, Train Acc: 10.02, Valid loss: 2.5411, Valid Acc: 10.34\n",
            "Epoch: 0, Train loss: 1.1198, Train Acc: 60.15, Valid loss: 1.2025, Valid Acc: 57.61\n",
            "Epoch: 1, Train loss: 0.8919, Train Acc: 69.27, Valid loss: 1.0390, Valid Acc: 63.39\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}